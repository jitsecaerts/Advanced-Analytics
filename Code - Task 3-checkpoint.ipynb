{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "import torch\n",
    "\n",
    "from pyspark.sql.functions import udf, concat_ws, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SPARK_HOME: C:\\Users\\topsj\\Desktop\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Using HADOOP_HOME: C:\\Users\\topsj\\Desktop\\spark\\winutils\n"
     ]
    }
   ],
   "source": [
    "spark_home = os.environ.get(\"SPARK_HOME\") or os.path.abspath(os.path.join(os.getcwd(), \"..\", \"spark-3.5.5-bin-hadoop3\"))\n",
    "hadoop_home = os.environ.get(\"HADOOP_HOME\") or os.path.abspath(os.path.join(os.getcwd(), \"..\", \"winutils\"))\n",
    "\n",
    "if not os.path.exists(spark_home):\n",
    "    print(f\"ERROR: SPARK_HOME does not exist: {spark_home}\")\n",
    "    exit(1)\n",
    "\n",
    "if os.name == \"nt\" and os.path.exists(hadoop_home):\n",
    "    os.environ[\"HADOOP_HOME\"] = hadoop_home\n",
    "    os.environ[\"PATH\"] = f\"{os.path.join(hadoop_home, 'bin')};{os.environ['PATH']}\"\n",
    "\n",
    "print(f\"Using SPARK_HOME: {spark_home}\")\n",
    "print(f\"Using HADOOP_HOME: {hadoop_home}\")\n",
    "\n",
    "findspark.init(spark_home)\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"StreamingPaperClassifier\")\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"StreamingPaperClassifierSession\").config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-verbose:gc\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables and model load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Global Variables --------------------\n",
    "\n",
    "globals().update({\n",
    "    'models_loaded': False,\n",
    "    'tokenizer': None,\n",
    "    'my_model': None,\n",
    "    'category_labels': []\n",
    "})\n",
    "\n",
    "# -------------------- Model Load Function --------------------\n",
    "def load_model_and_labels():\n",
    "    from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "    if globals()['models_loaded']:\n",
    "        return\n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        model_path = \"results/checkpoint-2631\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model path not found: {model_path}\")\n",
    "            return\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "\n",
    "        id2label = getattr(model.config, 'id2label', {})\n",
    "        if id2label:\n",
    "            try:\n",
    "                sorted_labels = [id2label[str(i)] for i in range(len(id2label))]\n",
    "            except Exception:\n",
    "                sorted_labels = list(id2label.values())\n",
    "\n",
    "            globals().update({\n",
    "                'tokenizer': tokenizer,\n",
    "                'my_model': model,\n",
    "                'category_labels': sorted_labels,\n",
    "                'models_loaded': True\n",
    "            })\n",
    "            print(f\"Model loaded with labels: {sorted_labels}\")\n",
    "        else:\n",
    "            print(\"WARNING: No id2label found in config. Predictions will be indices.\")\n",
    "            globals().update({\n",
    "                'tokenizer': tokenizer,\n",
    "                'my_model': model,\n",
    "                'models_loaded': True\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(text_list):\n",
    "    if not globals()['models_loaded']:\n",
    "        print(\"Model not loaded. Skipping inference.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        tokens = globals()['tokenizer'](text_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = globals()['my_model'](**tokens)\n",
    "            return torch.argmax(outputs.logits, axis=1).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize category function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_category_func(category_str):\n",
    "    if category_str is None:\n",
    "        return None\n",
    "    if \"q-fin\" in category_str:\n",
    "        return \"q-fin\"\n",
    "    elif \"q-bio\" in category_str:\n",
    "        return \"q-bio\"\n",
    "    return re.split(r'[-\\.]', category_str)[0]\n",
    "\n",
    "normalize_category_udf = udf(normalize_category_func, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json schema we follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Schema --------------------\n",
    "json_schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"published\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(time, rdd):\n",
    "    print(f\"\\n=== Processing batch at {time} ===\")\n",
    "\n",
    "    if not globals()['models_loaded']:\n",
    "        load_model_and_labels()\n",
    "        if not globals()['models_loaded']:\n",
    "            print(\"Model not loaded. Skipping batch.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        if rdd.isEmpty():\n",
    "            print(\"Empty RDD. Skipping.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        df = spark.read.json(rdd, schema=json_schema)\n",
    "        if df.rdd.isEmpty():\n",
    "            print(\"Parsed DataFrame is empty.\")\n",
    "            return\n",
    "\n",
    "        df = df.withColumn(\"normalized_main_category\", normalize_category_udf(col(\"main_category\")))\n",
    "        df = df.withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"summary\")))\n",
    "        df_processed = df.select(\"text\", \"normalized_main_category\").dropna()\n",
    "\n",
    "        if df_processed.rdd.isEmpty():\n",
    "            print(\"No valid rows after preprocessing.\")\n",
    "            return\n",
    "\n",
    "        pandas_df = df_processed.toPandas()\n",
    "        if pandas_df.empty:\n",
    "            print(\"Empty pandas DataFrame.\")\n",
    "            return\n",
    "\n",
    "        predictions = run_inference(pandas_df[\"text\"].tolist())\n",
    "        if not predictions or len(predictions) != len(pandas_df):\n",
    "            print(\"Mismatch or no predictions.\")\n",
    "            return\n",
    "\n",
    "        pandas_df[\"predicted_category_idx\"] = predictions\n",
    "        labels = globals()['category_labels']\n",
    "\n",
    "        if labels:\n",
    "            pandas_df[\"predicted_category_name\"] = pandas_df[\"predicted_category_idx\"].apply(\n",
    "                lambda idx: labels[idx] if 0 <= idx < len(labels) else f\"unknown_idx_{idx}\"\n",
    "            )\n",
    "        else:\n",
    "            pandas_df[\"predicted_category_name\"] = pandas_df[\"predicted_category_idx\"].astype(str)\n",
    "\n",
    "        correct = (pandas_df[\"normalized_main_category\"] == pandas_df[\"predicted_category_name\"]).sum()\n",
    "        total = len(pandas_df)\n",
    "        accuracy = correct / total if total else 0\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        print(\"Sample Predictions:\")\n",
    "        print(pandas_df[[\"normalized_main_category\", \"predicted_category_name\"]].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Batch processing error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\topsj\\Desktop\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing batch at 2025-05-16 20:47:30 ===\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threading for streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc_instance):\n",
    "        super().__init__()\n",
    "        self.ssc_instance = ssc_instance\n",
    "        self._stop_event = threading.Event()\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            print(\"Starting streaming context...\")\n",
    "            self.ssc_instance.start()\n",
    "            while not self._stop_event.is_set():\n",
    "                self._stop_event.wait(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            self.ssc_instance.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping streaming context...\")\n",
    "        self._stop_event.set()\n",
    "        if self.ssc_instance.getState() == StreamingContext.STATE_ACTIVE:\n",
    "            self.ssc_instance.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting streaming context...Streaming started. Press Ctrl+C to stop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ssc_t = StreamingThread(ssc)\n",
    "    try:\n",
    "        ssc_t.start()\n",
    "        print(\"Streaming started. Press Ctrl+C to stop.\")\n",
    "        while ssc_t.is_alive():\n",
    "            ssc_t.join(timeout=1.0)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboardInterrupt received. Stopping...\")\n",
    "    finally:\n",
    "        if ssc_t.is_alive():\n",
    "            ssc_t.stop_stream()\n",
    "            ssc_t.join(timeout=BATCH_INTERVAL_SECONDS * 2 + 5)\n",
    "        print(\"Shutdown complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
