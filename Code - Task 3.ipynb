{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "import torch\n",
    "\n",
    "from pyspark.sql.functions import udf, concat_ws, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   value   8763 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 68.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                value\n",
       " 0  {\"aid\": \"http://arxiv.org/abs/2504.09896v1\", \"...\n",
       " 1  {\"aid\": \"http://arxiv.org/abs/2504.09912v1\", \"...\n",
       " 2  {\"aid\": \"http://arxiv.org/abs/2504.04742v1\", \"...\n",
       " 3  {\"aid\": \"http://arxiv.org/abs/2504.04758v1\", \"...\n",
       " 4  {\"aid\": \"http://arxiv.org/abs/2504.04721v1\", \"...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"C:/Users/siebe/Dropbox/KUL/1ste master/Advanced Analytics in Business/assignements/assignement 3/output (1).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show basic info and the first few rows\n",
    "df.info(), df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   aid            8763 non-null   object\n",
      " 1   title          8763 non-null   object\n",
      " 2   summary        8763 non-null   object\n",
      " 3   main_category  8763 non-null   object\n",
      " 4   categories     8763 non-null   object\n",
      " 5   published      8763 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 410.9+ KB\n",
      "8763\n",
      "Number of unique main categories: 15\n",
      "unique categories: ['cs' 'eess' 'astro' 'physics' 'quant' 'math' 'cond' 'nucl' 'stat' 'hep'\n",
      " 'q-bio' 'gr' 'q-fin' 'econ' 'nlin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Parse the JSON strings in the 'value' column\n",
    "parsed_rows = df['value'].apply(json.loads)\n",
    "# Convert to a DataFrame\n",
    "structured_df = pd.json_normalize(parsed_rows)\n",
    "# Show the structure and a sample\n",
    "structured_df.info(), structured_df.head()\n",
    "\n",
    "import sklearn\n",
    "import re\n",
    "# Normalize all main_category values to their top-level prefix (before '-' or '.')\n",
    "structured_df['main_category'] = structured_df['main_category'].apply(lambda x: 'q-fin' if 'q-fin' in x else ('q-bio' if 'q-bio' in x else re.split(r'[-\\.]', x)[0]))\n",
    "\n",
    "structured_df['text'] = structured_df['title'] + \" \" + structured_df['summary']\n",
    "structured_df = structured_df[['text', 'main_category']].dropna()\n",
    "print(len(structured_df))\n",
    "\n",
    "# Encode main_category labels as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "structured_df['label'] = label_encoder.fit_transform(structured_df['main_category'])\n",
    "\n",
    "# Get a summary of label distribution\n",
    "label_distribution = structured_df['main_category'].value_counts()\n",
    "\n",
    "# Show sample data and label distribution\n",
    "structured_df.head(), label_distribution.head(10), structured_df['label'].nunique()\n",
    "\n",
    "# Get the number of unique main categories\n",
    "num_unique_categories = structured_df['main_category'].nunique()\n",
    "print(f'Number of unique main categories: {num_unique_categories}')\n",
    "\n",
    "# Get the unique main categories --> this is to check whether the categories are transformed into their desired form: \n",
    "# i.e all main_category values must be transformed to their top-level prefix since that is what we want to predict\n",
    "unique_categories = structured_df['main_category'].unique()\n",
    "print(f'unique categories: {unique_categories}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Set up and train the DistelBERT base uncased model. This is the transformer-based language model we will use for prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Hugging Face Datasets & Transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Step 1: Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "num_labels = len(structured_df['main_category'].unique())\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "train_df, test_df = train_test_split(structured_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Step 4: Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Step 5: Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Step 6: Define metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), axis=1)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Step 7: Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Step 8: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Checkpoint: When fine-tuning a Hugging Face model using the Trainer API (e.g. trainer.train()), it automatically saves checkpoints during training at regular intervals. Each checkpoint-* directory is a snapshot of the model at a certain training step. So in order to load in the model in the streaming environment, we need to check which checkpoint to use. This is the purpose of the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint-1374', 'checkpoint-1754', 'checkpoint-2631', 'checkpoint-458', 'checkpoint-877', 'checkpoint-916']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"./results\"\n",
    "print(os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to find the best checkpoint such that I know which checkpoint to load in into the deployed setting\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "num_labels = len(structured_df['main_category'].unique())\n",
    "\n",
    "train_df, test_df = train_test_split(structured_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#Loop over all checkpoints and evaluate agaist the defined metrics\n",
    "results = []\n",
    "checkpoints_dir = \"./results\"\n",
    "checkpoints = [os.path.join(checkpoints_dir, d) for d in os.listdir(checkpoints_dir) if d.startswith(\"checkpoint\")]\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Evaluating {checkpoint}...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    results.append((checkpoint, eval_result))\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x[1][\"eval_loss\"])\n",
    "best_checkpoint, best_result = sorted_results[0]\n",
    "\n",
    "print(f\"\\nBeste checkpoint: {best_checkpoint}\")\n",
    "print(f\"Resultaten: {best_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SPARK_HOME: C:\\Users\\topsj\\Desktop\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Using HADOOP_HOME: C:\\Users\\topsj\\Desktop\\spark\\winutils\n"
     ]
    }
   ],
   "source": [
    "spark_home = os.environ.get(\"SPARK_HOME\") or os.path.abspath(os.path.join(os.getcwd(), \"..\", \"spark-3.5.5-bin-hadoop3\"))\n",
    "hadoop_home = os.environ.get(\"HADOOP_HOME\") or os.path.abspath(os.path.join(os.getcwd(), \"..\", \"winutils\"))\n",
    "\n",
    "if not os.path.exists(spark_home):\n",
    "    print(f\"ERROR: SPARK_HOME does not exist: {spark_home}\")\n",
    "    exit(1)\n",
    "\n",
    "if os.name == \"nt\" and os.path.exists(hadoop_home):\n",
    "    os.environ[\"HADOOP_HOME\"] = hadoop_home\n",
    "    os.environ[\"PATH\"] = f\"{os.path.join(hadoop_home, 'bin')};{os.environ['PATH']}\"\n",
    "\n",
    "print(f\"Using SPARK_HOME: {spark_home}\")\n",
    "print(f\"Using HADOOP_HOME: {hadoop_home}\")\n",
    "\n",
    "findspark.init(spark_home)\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"StreamingPaperClassifier\")\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"StreamingPaperClassifierSession\").config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-verbose:gc\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Global variables and model load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Global Variables --------------------\n",
    "\n",
    "globals().update({\n",
    "    'models_loaded': False,\n",
    "    'tokenizer': None,\n",
    "    'my_model': None,\n",
    "    'category_labels': []\n",
    "})\n",
    "\n",
    "# -------------------- Model Load Function --------------------\n",
    "def load_model_and_labels():\n",
    "    from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "    if globals()['models_loaded']:\n",
    "        return\n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        model_path = \"results/checkpoint-2631\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model path not found: {model_path}\")\n",
    "            return\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "\n",
    "        id2label = getattr(model.config, 'id2label', {})\n",
    "        if id2label:\n",
    "            try:\n",
    "                sorted_labels = [id2label[str(i)] for i in range(len(id2label))]\n",
    "            except Exception:\n",
    "                sorted_labels = list(id2label.values())\n",
    "\n",
    "            globals().update({\n",
    "                'tokenizer': tokenizer,\n",
    "                'my_model': model,\n",
    "                'category_labels': sorted_labels,\n",
    "                'models_loaded': True\n",
    "            })\n",
    "            print(f\"Model loaded with labels: {sorted_labels}\")\n",
    "        else:\n",
    "            print(\"WARNING: No id2label found in config. Predictions will be indices.\")\n",
    "            globals().update({\n",
    "                'tokenizer': tokenizer,\n",
    "                'my_model': model,\n",
    "                'models_loaded': True\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(text_list):\n",
    "    if not globals()['models_loaded']:\n",
    "        print(\"Model not loaded. Skipping inference.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        tokens = globals()['tokenizer'](text_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = globals()['my_model'](**tokens)\n",
    "            return torch.argmax(outputs.logits, axis=1).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Normalize category function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_category_func(category_str):\n",
    "    if category_str is None:\n",
    "        return None\n",
    "    if \"q-fin\" in category_str:\n",
    "        return \"q-fin\"\n",
    "    elif \"q-bio\" in category_str:\n",
    "        return \"q-bio\"\n",
    "    return re.split(r'[-\\.]', category_str)[0]\n",
    "\n",
    "normalize_category_udf = udf(normalize_category_func, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Json schema we follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Schema --------------------\n",
    "json_schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"published\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(time, rdd):\n",
    "    print(f\"\\n=== Processing batch at {time} ===\")\n",
    "\n",
    "    if not globals()['models_loaded']:\n",
    "        load_model_and_labels()\n",
    "        if not globals()['models_loaded']:\n",
    "            print(\"Model not loaded. Skipping batch.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        if rdd.isEmpty():\n",
    "            print(\"Empty RDD. Skipping.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        df = spark.read.json(rdd, schema=json_schema)\n",
    "        if df.rdd.isEmpty():\n",
    "            print(\"Parsed DataFrame is empty.\")\n",
    "            return\n",
    "\n",
    "        df = df.withColumn(\"normalized_main_category\", normalize_category_udf(col(\"main_category\")))\n",
    "        df = df.withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"summary\")))\n",
    "        df_processed = df.select(\"text\", \"normalized_main_category\").dropna()\n",
    "\n",
    "        if df_processed.rdd.isEmpty():\n",
    "            print(\"No valid rows after preprocessing.\")\n",
    "            return\n",
    "\n",
    "        pandas_df = df_processed.toPandas()\n",
    "        if pandas_df.empty:\n",
    "            print(\"Empty pandas DataFrame.\")\n",
    "            return\n",
    "\n",
    "        predictions = run_inference(pandas_df[\"text\"].tolist())\n",
    "        if not predictions or len(predictions) != len(pandas_df):\n",
    "            print(\"Mismatch or no predictions.\")\n",
    "            return\n",
    "\n",
    "        pandas_df[\"predicted_category_idx\"] = predictions\n",
    "        labels = globals()['category_labels']\n",
    "\n",
    "        if labels:\n",
    "            pandas_df[\"predicted_category_name\"] = pandas_df[\"predicted_category_idx\"].apply(\n",
    "                lambda idx: labels[idx] if 0 <= idx < len(labels) else f\"unknown_idx_{idx}\"\n",
    "            )\n",
    "        else:\n",
    "            pandas_df[\"predicted_category_name\"] = pandas_df[\"predicted_category_idx\"].astype(str)\n",
    "\n",
    "        correct = (pandas_df[\"normalized_main_category\"] == pandas_df[\"predicted_category_name\"]).sum()\n",
    "        total = len(pandas_df)\n",
    "        accuracy = correct / total if total else 0\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        print(\"Sample Predictions:\")\n",
    "        print(pandas_df[[\"normalized_main_category\", \"predicted_category_name\"]].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Batch processing error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\topsj\\Desktop\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing batch at 2025-05-16 20:47:30 ===\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Threading for streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc_instance):\n",
    "        super().__init__()\n",
    "        self.ssc_instance = ssc_instance\n",
    "        self._stop_event = threading.Event()\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            print(\"Starting streaming context...\")\n",
    "            self.ssc_instance.start()\n",
    "            while not self._stop_event.is_set():\n",
    "                self._stop_event.wait(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            self.ssc_instance.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "    def stop_stream(self):\n",
    "        print(\"Stopping streaming context...\")\n",
    "        self._stop_event.set()\n",
    "        if self.ssc_instance.getState() == StreamingContext.STATE_ACTIVE:\n",
    "            self.ssc_instance.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting streaming context...Streaming started. Press Ctrl+C to stop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ssc_t = StreamingThread(ssc)\n",
    "    try:\n",
    "        ssc_t.start()\n",
    "        print(\"Streaming started. Press Ctrl+C to stop.\")\n",
    "        while ssc_t.is_alive():\n",
    "            ssc_t.join(timeout=1.0)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboardInterrupt received. Stopping...\")\n",
    "    finally:\n",
    "        if ssc_t.is_alive():\n",
    "            ssc_t.stop_stream()\n",
    "            ssc_t.join(timeout=BATCH_INTERVAL_SECONDS * 2 + 5)\n",
    "        print(\"Shutdown complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
